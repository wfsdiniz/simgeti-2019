{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construindo uma RNA para reconhecimento de dígitos\n",
    "\n",
    "Agora que conhecemos os conceitos teóricos das RNAs, vamos colocá-los em prática, construindo uma RNA para o reconhecimento de dígitos manuscritos e digitalizados.  Por que vamos começar com este tipo de tarefa? Porque ela permite percorrer todas as etapas e procedimentos matemáticos de uma rede neural, sendo portanto uma excelente introdução.\n",
    "\n",
    "Antes de pensar em escrever sua primeira linha de código, é preciso definir claramente o problema a ser resolvido. A tecnologia existe para resolver problemas e a definição clara do objetivo é o ponto de partida de qualquer projeto de sucesso. Desta forma, vamos começar definindo os dados de entrada do classificador.\n",
    "\n",
    "Podemos dividir o problema de reconhecer os dígitos manuscritos em dois sub-problemas. Primeiro, precisamos encontrar uma maneira de quebrar uma imagem que contenha muitos dígitos em uma sequência de imagens separadas, cada uma contendo um único dígito. Por exemplo, gostaríamos de quebrar a imagem:\n",
    "\n",
    "![](rawdigits.png)\n",
    "\n",
    "Em dígitos individuais, mais ou menos assim:\n",
    "\n",
    "![](digits_separate.png)\n",
    "\n",
    "Humanos resolvem esse problema de segmentação com facilidade, mas é um desafio para um programa de computador dividir corretamente a imagem. Uma vez que a imagem foi segmentada, o programa precisa classificar cada dígito individual. Então, por exemplo, gostaríamos que nosso programa reconhecesse automaticamente que o primeiro dígito acima é um 5.\n",
    "\n",
    "Vamos nos concentrar em escrever um programa para resolver o segundo problema, isto é, classificar dígitos individuais. O problema da segmentação não é tão difícil de resolver, uma vez que você tenha uma boa maneira de classificar os dígitos individuais. Existem muitas abordagens para resolver o problema de segmentação. Uma abordagem é testar muitas maneiras diferentes de segmentar a imagem, usando o classificador de dígitos individuais para marcar cada segmentação de teste. Uma segmentação de teste obtém uma pontuação alta se o classificador de dígitos individuais estiver confiante de sua classificação em todos os segmentos e uma pontuação baixa se o classificador tiver muitos problemas em um ou mais segmentos. A ideia é que, se o classificador estiver tendo problemas em algum lugar, provavelmente está tendo problemas porque a segmentação foi escolhida incorretamente. Essa ideia e outras variações podem ser usadas para resolver o problema de segmentação. Então, em vez de se preocupar com a segmentação, nos concentraremos no desenvolvimento de uma rede neural que pode resolver o problema mais interessante e difícil, ou seja, reconhecer dígitos individuais manuscritos.\n",
    "\n",
    "Para reconhecer dígitos individuais, usaremos uma rede neural de três camadas. Vamos começar definindo que cada digito será segmentado em uma imagem com $28 \\times 28$ pixels, de forma a resultar em um total de $784$ pixels por imagem. Vamos definir nossa camada de entrada mapeando cada pixel, ou seja, teremos 784 neurônios na primeira camada. O valor de entrada de cada neurônio será a cor do pixel correspondente em escala de cinza, devendo variar entre 0.0 e 1.0, onde 0.0 representa a cor branca e 1.0 a cor preta e valores entre estes, gradações de cinza.\n",
    "\n",
    "Em seguida, adicionaremos uma camada oculta, cujo número de neurônios, chamaremos de $n$. Vamos testar várias configurações de $n$, para observar qual dará o melhor resultado. Vamos definir arbitrariamente o valor $n = 15$, inicialmente.\n",
    "\n",
    "Finalmente, vamos definir nossa camada de saída. Como temos 10 possíveis dígitos (0 a 9), vamos também usar 10 neurônios na camada de saída. Numeraremos cada neurônio, de forma que, se, por exemplo, o neurônio 3 possua o maior sinal de ativação, significa que o dígito foi reconhecido como um 3 e assim por diante. A definição da quantidade de neurônios de saída não segue uma regra simples, mas é sim baseada em herísticas que foram definidas empiricamente. Por exemplo, podemos pensar que 4 neurônios de saída seriam suficientes para codificar os 10 dígitos, uma vez que $2^4 = 16$. Entretanto, verifica-se que para este caso, uma rede com 10 neurônios de saída aprende melhor do que uma com 4. A justificativa final é empírica: podemos experimentar ambos os projetos de rede, e verifica-se que, para este problema específico, a rede com 10 neurônios de saída aprende a reconhecer dígitos melhor do que a rede com 4 neurônios de saída. Mas isso ainda deixa a pergunta por que o uso de 10 neurônios de saída funciona melhor. Por exemplo, podemos ter certos neurônios na camada oculta ligados ao reconhecimento de partes individuais de cada dígito. A combinação da ativação desses neurônios dá o dígito reconhecido.\n",
    "\n",
    "A rede resultante terá esta configuração:\n",
    "\n",
    "![](digits_arch_black.png)\n",
    "\n",
    "Agora que já temos a arquitetura da rede neural, precisamos definir como será o processo de aprendizagem do algoritmo, antes de começar a codificar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizado com gradiente descendente\n",
    "\n",
    "A primeira coisa que precisamos é um conjunto de dados para o treinamento da rede. Usaremos o conjunto de dados [MNIST](http://yann.lecun.com/exdb/mnist/), que contém dezenas de milhares de imagens digitalizadas de dígitos manuscritos, juntamente com suas classificações corretas. O nome MNIST vem do fato de que é um subconjunto modificado de dois conjuntos de dados coletados pelo NIST, o Instituto Nacional de Padrões e Tecnologia dos Estados Unidos. Aqui estão algumas imagens do MNIST:\n",
    "\n",
    "![](digits_separate.png)\n",
    "\n",
    "Estes dados foram coletados a partir da digitalização de manuscritos de 250 pessoas, metade funcionários do Bureau do Censo dos EUA e metade estudantes do ensino médio. As imagens estão em escala de cinza e 28 por 28 pixels de tamanho. A base está dividida em duas partes, sendo a primeira um conjunto de treinamento com 60k amostras e a segunda um conjunto de teste com 10k amostras. O conjunto de teste é formado por coletas de 250 pessoas diferentes em relação às que participam do conjunto de treinamento. Desta forma, garantimos que a rede consegue reconhecer uma caligrafia que não viu durante o treinamento (que é o que desejamos que ela faça!).\n",
    "\n",
    "Usaremos a notação x para indicar uma entrada (input) de treinamento. Será conveniente considerar cada entrada de treinamento x (cada imagem) como um vetor de 784 posições (28 x 28 pixels). A imagem abaixo representa como este vetor é construído:\n",
    "\n",
    "![](pixels.png)\n",
    "\n",
    "Cada entrada no vetor representa o valor de cinza para um único pixel na imagem. Vamos indicar a saída correspondente desejada por $y = y(x)$, onde $y$ é um vetor com dimensão 10. Por exemplo, se uma imagem de treinamento particular, $x$, representa um 3, então $y(x) = (0,0,0,1,0,0,0,0,0,0)^T$ é a saída desejada da rede . Observe que T aqui é a operação de transposição, transformando um vetor de linha em um vetor comum (coluna).\n",
    "\n",
    "O que queremos é um algoritmo que nos permita encontrar pesos e bias para que a saída da rede se aproxime de $y(x)$ para todas as entradas de treinamento $x$. Para quantificar o quão bem estamos alcançando esse objetivo, precisamos definir uma função de custo, como esta:\n",
    "\n",
    "$$\n",
    "C(w,b) \\equiv \\frac{1}{2n}\\sum^{ }_{x}{\\left \\| y(x) - a \\right \\|}^2\n",
    "$$\n",
    "\n",
    "Na fórmula acima, $w$ indica a coleta de todos os pesos na rede, $b$ todos os bias (viés), $n$ é o número total de entradas de treinamento, $a$ é o vetor de saídas da rede (quando $x$ é entrada, ou seja o valor desejado) e a soma é sobre todas as entradas de treinamento $x$.\n",
    "\n",
    "A notação $\\left \\| v \\right \\|$ apenas indica a função de comprimento usual para um vetor $v$. Chamaremos $C$ a função de custo quadrático, que também é conhecido como o erro quadrático médio ou apenas o MSE (Mean Squared Error). Inspecionando a forma da função de custo quadrático, vemos que $C (w, b)$ não é negativo, pois cada termo na soma não é negativo. Além disso, o custo $C (w, b)$ torna-se pequeno, isto é, $C (w, b) ≈ 0$, precisamente quando $y(x)$ é aproximadamente igual à saída, a, para todas as entradas de treinamento $x$.\n",
    "\n",
    "Portanto, nosso algoritmo de treinamento faz um bom trabalho se ele pode encontrar um conjunto de pesos e bias que faça com que $C (w, b) ≈ 0$. Isso significa basicamente que nosso modelo fez as previsões corretas, ou seja, cada vez que apresentamos ao modelo uma imagem com dígito 3, ele é capaz de reconhecer que se trata do número 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O Gradiente Descendente\n",
    "\n",
    "A maioria das tarefas em Aprendizado de Máquina são na verdade problemas de otimização e um dos algoritmos mais usados para isso é o Algoritmo de Descida do Gradiente (ou Gradiente Descendente). Para um iniciante, o nome Algoritmo de Descida do Gradiente pode parecer intimidante, mas espero que depois de ler o que está logo abaixo, isso deixe de ser um mistério para você.\n",
    "\n",
    "A Descida do Gradiente é uma ferramenta padrão para otimizar funções complexas iterativamente dentro de um programa de computador. Seu objetivo é: dada alguma função arbitrária, encontrar um mínimo. Para alguns pequenos subconjuntos de funções – aqueles que são convexos – há apenas um único mínimo que também acontece de ser global. Para as funções mais realistas, pode haver muitos mínimos, então a maioria dos mínimos são locais. Certifique-se de que a otimização encontre o “melhor” mínimo e não fique preso em mínimos sub-otimistas (um problema comum durante o treinamento do algoritmo).\n",
    "\n",
    "Para compreender a intuição da Descida do Gradiente, vamos simplificar um pouco as coisas. Vamos imaginar que simplesmente recebemos uma função de muitas variáveis e queremos minimizar essa função.\n",
    "\n",
    "Há uma analogia que nos ajuda a compreender como encontrar a solução. Começamos por pensar em nossa função como uma espécie de vale e imaginamos uma bola rolando pela encosta do vale, conforme pode ser visto na figura abaixo. Nossa experiência diária nos diz que a bola acabará rolando para o fundo do vale. Talvez possamos usar essa ideia como forma de encontrar um mínimo para a função? Escolheríamos aleatoriamente um ponto de partida para uma bola (imaginária), e então simularíamos o movimento da bola enquanto ela rola até o fundo do vale. Poderíamos fazer essa simulação simplesmente por derivadas de computação da função C – essas derivadas nos diriam tudo o que precisamos saber sobre a “forma” local do vale, e, portanto, como nossa bola deve rolar.\n",
    "\n",
    "![](ball.png)\n",
    "\n",
    "Ou seja, a Descida do Gradiente é um algoritmo de otimização usado para encontrar os valores de parâmetros (coeficientes ou se preferir w e b – weight e bias) de uma função que minimizam uma função de custo. A Descida do Gradiente é melhor usada quando os parâmetros não podem ser calculados analiticamente (por exemplo, usando álgebra linear) e devem ser pesquisados por um algoritmo de otimização.\n",
    "\n",
    "O procedimento começa com valores iniciais para o coeficiente ou coeficientes da função. Estes poderiam ser 0.0 ou um pequeno valor aleatório (a inicialização dos coeficiente é parte crítica do processo e diversas técnicas podem ser usadas, ficando a escolha a cargo do Cientista de Dados e do problema a ser resolvido com o modelo). Poderíamos iniciar assim nossos coeficientes (valores de w e b):\n",
    "\n",
    "$$\n",
    "w = 0.0\n",
    "$$\n",
    "\n",
    "O custo dos coeficientes é avaliado ligando-os à função e calculando o custo para uma entrada $x$. \n",
    "\n",
    "$$C(x_i) = f(coeficiente)$$\n",
    "\n",
    "ou\n",
    "\n",
    "$$C(x_i) = avaliar(f(coeficiente))$$\n",
    "\n",
    "A derivada do custo é calculada. A derivada é um conceito de Cálculo e refere-se à inclinação da função em um determinado ponto. Precisamos conhecer a inclinação para que possamos conhecer a direção (sinal) para mover os valores dos coeficientes para obter um custo menor na próxima iteração.\n",
    "\n",
    "$$\\delta = \\frac{dC(x_i)}{dx_i}$$ \n",
    "\n",
    "Agora que sabemos da derivada em que direção está em declive, podemos atualizar os valores dos coeficientes. Um parâmetro de taxa de aprendizagem (eta - $\\eta$) deve ser especificado e controla o quanto os coeficientes podem mudar em cada atualização.\n",
    "\n",
    "$$C(x_i) = coeficiente – (\\eta * \\delta)$$\n",
    "\n",
    "Este processo é repetido até que o custo dos coeficientes (função de custo) seja 0.0 ou próximo o suficiente de zero, indicando que as saídas da rede estão cada vez mais próximas dos valores reais (saídas desejadas).\n",
    "\n",
    "A Descida do Gradiente é simples, mas exige que seja calculado o gradiente da função de custo ou a função que você está otimizando, mas além disso, é muito direto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente descendente em lotes\n",
    "\n",
    "O objetivo de todos os algoritmos supervisionados de aprendizagem de máquina é estimar uma função de destino (f) que mapeia dados de entrada (X) para as variáveis de saída (Y). Isso descreve todos os problemas de classificação e regressão (aprendizagem supervisionada).\n",
    "\n",
    "Alguns algoritmos de aprendizagem de máquina têm coeficientes que caracterizam a estimativa de algoritmos para a função alvo (f). Diferentes algoritmos têm diferentes representações e diferentes coeficientes, mas muitos deles requerem um processo de otimização para encontrar o conjunto de coeficientes que resultam na melhor estimativa da função alvo. Os exemplos comuns de algoritmos com coeficientes que podem ser otimizados usando descida do gradiente são Regressão linear e Regressão logística.\n",
    "\n",
    "A avaliação de quão próximo um modelo de aprendizagem de máquina estima a função de destino pode ser calculada de várias maneiras, muitas vezes específicas para o algoritmo de aprendizagem de máquina. A função de custo envolve a avaliação dos coeficientes no modelo de aprendizagem de máquina calculando uma previsão para o modelo para cada instância de treinamento no conjunto de dados e comparando as previsões com os valores de saída reais e calculando uma soma ou erro médio (como a Soma de Residuais Quadrados ou SSR no caso de regressão linear).\n",
    "\n",
    "A partir da função de custo, uma derivada pode ser calculada para cada coeficiente para que ele possa ser atualizado usando exatamente a equação de atualização descrita acima.\n",
    "\n",
    "O custo é calculado para um algoritmo de aprendizado de máquina em todo o conjunto de dados de treinamento para cada iteração do algoritmo de descida de gradiente. Uma iteração do algoritmo é chamada de um lote e esta forma de descida do gradiente é referida como Descida do Gradiente em Lote (Batch Gradient Descent).\n",
    "\n",
    "A descida do gradiente em lote é a forma mais comum de descida do gradiente em Aprendizado de Máquina.\n",
    "\n",
    "### Descida do Gradiente Estocástica\n",
    "\n",
    "A Descida do Gradiente pode ser lenta para executar em conjuntos de dados muito grandes. Como uma iteração do algoritmo de descida do gradiente requer uma previsão para cada instância no conjunto de dados de treinamento, pode demorar muito quando você tem muitos milhões de instâncias.\n",
    "\n",
    "Em situações em que você possui grandes quantidades de dados, você pode usar uma variação da descida do gradiente chamada Stochastic Gradient Descent.\n",
    "\n",
    "Nesta variação, o procedimento de descida do gradiente descrito acima é executado, mas a atualização para os coeficientes é realizada para cada instância de treinamento, em vez do final do lote de instâncias.\n",
    "\n",
    "O primeiro passo do procedimento exige que a ordem do conjunto de dados de treinamento seja randomizada. Isto é, misturar a ordem que as atualizações são feitas para os coeficientes. Como os coeficientes são atualizados após cada instância de treinamento, as atualizações serão ruidosas, saltando por todo o lado, e assim o custo correspondente funcionará. Ao misturar a ordem para as atualizações dos coeficientes, ela aproveita essa caminhada aleatória e evita que ela fique “distraída” ou presa em um mínimo local.\n",
    "\n",
    "O procedimento de atualização para os coeficientes é o mesmo que o anterior, exceto que o custo não é somado em todos os padrões de treinamento, mas sim calculado para um padrão de treinamento.\n",
    "\n",
    "A aprendizagem pode ser muito mais rápida com descida de gradiente estocástica para conjuntos de dados de treinamento muito grandes e muitas vezes você só precisa de um pequeno número de passagens através do conjunto de dados para alcançar um conjunto de coeficientes bom o suficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação em Python\n",
    "\n",
    "Depois de toda essa teoria, é hora de ver na prática! Como citamos anteriormente, usaremos a bade de dados MNIST de dígitos nesse exercício. A base contém 60k imagens de treinamento e 10k imagens de teste. Podemos usá-la como está, mas vamos fazer nosso processo de forma um pouco diferente. Vamos dividir as 60k imagens de treinamento em um conjunto de 50k de treino e 10k para validação, que usaremos para validar o resultado e afinar outros parâmetros da rede, como a taxa de aprendizagem.\n",
    "\n",
    "Vamos utilizar o Numpy para lidar com os valores. Comecemos definindo a classe RedeNeural:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class RedeNeural:\n",
    "    def __init__(self, tamanhos):\n",
    "        self.ncamadas = len(tamanhos)\n",
    "        self.tamanhos = tamanhos\n",
    "        self.bias  = [np.random.randn(y, 1) for y in tamanhos[1:]]\n",
    "        self.pesos = [np.random.randn(y, x) for x, y in zip(tamanhos[:-1], tamanhos[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste código, o parâmetro *tamanhos* contêm o número de neurônios nas respectivas camadas, sendo um objeto do tipo lista em Python. Então, por exemplo, se queremos criar um objeto da classe RedeNeural com 2 neurônios na primeira camada, 3 neurônios na segunda camada e 1 neurônio na camada final, aqui está o código que usamos para instanciar um objeto da classe\n",
    "\n",
    "$$rede = RedeNeural([2,3,1])$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias:  [array([[-1.12295494],\n",
      "       [-1.63789129],\n",
      "       [-0.1914761 ]]), array([[-0.58033393]])]\n",
      "Pesos:  [array([[2.06608469, 0.47819627],\n",
      "       [0.04202737, 0.05670925],\n",
      "       [0.2004814 , 1.27357825]]), array([[ 1.58789921, -0.08455679,  0.09899815]])]\n"
     ]
    }
   ],
   "source": [
    "rede = RedeNeural([2,3,1])\n",
    "print(\"Bias: \", rede.bias)\n",
    "print(\"Pesos: \", rede.pesos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os pesos e bias são inicializados aleatoriamente, usando a função *np.random.randn*, que gera números com distribuição gaussiana com média 0 e desvio padrão 1. Usaremos o algoritmo de descida de gradiente estocástico, usando esse chute inicial aleatório. Observe que a primeira camada não faz nenhuma computação, então não tem nenhum valor de bias.\n",
    "\n",
    "Agora, vamos definir a função de ativação da rede. Usaremos a função sigmoide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sigmoide(self, z):\n",
    "        return 1./(1. + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que quando a entrada z é um vetor ou uma matriz Numpy, Numpy aplica automaticamente a função sigmoid elementwise, ou seja, na forma vetorizada.\n",
    "\n",
    "Em seguida, adicionamos um método feedforward à classe Network, que, dada a entrada a para a rede, retorna a saída corresponente. Basicamente o método feedforward aplica a Equação 1 mostrada acima, para cada camada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def feedForward(self, a):\n",
    "        for b, w in zip(self.bias, self.pesos):\n",
    "            a = self.sigmoide(np.dot(w, a)+b)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A principal atividade que queremos que nossos objetos da classe Network façam é aprender. Para esse fim, criaremos um método SGD (Stochastic Gradient Descent). Aqui está o código. É um pouco misterioso em alguns lugares, mas vamos explicar em detalhes mais abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def SGD(self, treino, epocas, tamanho_lote, eta, teste=None):\n",
    "        treino = list(treino)\n",
    "        n = len(treino)\n",
    "        \n",
    "        if teste:\n",
    "            teste = list(teste)\n",
    "            n_teste = len(teste)\n",
    "        \n",
    "        for j in range(epocas):\n",
    "            random.shuffle(treino)\n",
    "            lotes = [treino[k:k+tamanho_lote] for k in range(0, n, tamanho_lote)]\n",
    "            \n",
    "            for lote in lotes:\n",
    "                self.atualiza_lote(lote, eta)\n",
    "                \n",
    "            if teste:\n",
    "                print(\"Época {} : {}/{}\".format(j, self.avalia(teste), n_teste));\n",
    "            else:\n",
    "                print(\"Época {} finalizada\".format(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O parâmetro *treino* é uma lista de tuplas (x, y) que representam as entradas de treinamento e as correspondentes saídas desejadas. As variáveis *epocas* e *tamanho_lote* são o que você esperaria – o número de épocas para treinar e o tamanho dos mini-lotes a serem usados durante a amostragem, enquanto *eta* é a taxa de aprendizagem, η. Se o argumento opcional *teste* for fornecido, o programa avaliará a rede após cada período de treinamento e imprimirá o progresso parcial. Isso é útil para rastrear o progresso, mas retarda substancialmente as coisas.\n",
    "\n",
    "O código funciona da seguinte forma. Em cada época, ele começa arrastando aleatoriamente os dados de treinamento e, em seguida, particiona-os em mini-lotes de tamanho apropriado. Esta é uma maneira fácil de amostragem aleatória dos dados de treinamento. Então, para cada lote, aplicamos um único passo de descida do gradiente. Isso é feito pelo código *self.atualiza_lote(lote, eta)*, que atualiza os pesos e os bias da rede de acordo com uma única iteração de descida de gradiente, usando apenas os dados de treinamento em mini-lotes. Aqui está o código para o método atualiza_lote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def atualiza_lote(self, lote, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.bias]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.pesos]\n",
    "        \n",
    "        for x, y in lote:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        self.pesos = [w-(eta/len(lote))*nw for w, nw in zip(self.pesos, nabla_w)]\n",
    "        self.bias  = [b-(eta/len(lote))*nb for b, nb in zip(self.bias, nabla_b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A maior parte do trabalho é feita pela linha delta_nabla_b, delta_nabla_w = self.backprop (x, y). Isso invoca o algoritmo de backpropagation, que é uma maneira rápida de calcular o gradiente da função de custo. Portanto, *atualiza_lote* funciona simplesmente calculando esses gradientes para cada exemplo de treinamento no lote e, em seguida, atualizando *self.pesos* e *self.biases* adequadamente.\n",
    "\n",
    "Abaixo você encontra o código para *self.backprop*, mas não o estudaremos agora. Estudaremos em detalhes como funciona o backpropagation futuramente. Por hora, basta assumir que ele se comporta conforme indicado, retornando o gradiente apropriado para o custo associado ao exemplo de treinamento *x*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.bias]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.pesos]\n",
    "        \n",
    "        # Feedforward\n",
    "        ativacao = x\n",
    "        \n",
    "        # Lista para armazenar todas as ativações, camada por camada\n",
    "        ativacoes = [x]\n",
    "        \n",
    "        # Lista par armazenar todos os vetores z, camada por camada\n",
    "        zs = []\n",
    "        \n",
    "        for b, w in zip(self.bias, self.pesos):\n",
    "            z = np.dot(w, ativacao) + b\n",
    "            zs.append(z)\n",
    "            ativacao = self.sigmoide(z)\n",
    "            ativacoes.append(ativacao)\n",
    "        \n",
    "        # Backward pass\n",
    "        delta = self.derivada_custo(ativacoes[-1], y) * self.sigmoide_prim(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, ativacoes[-2].transpose())\n",
    "        \n",
    "        for l in range(2, self.ncamadas):\n",
    "            z = zs[-l]\n",
    "            sp = self.sigmoide_prim(z)\n",
    "            delta = np.dot(self.pesos[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, ativacoes[-l+1].transpose()) # Esta linha estava errada\n",
    "        return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe **RedeNeural** é em essência nosso algoritmo de rede neural. A partir dela criamos uma instância (como rede), alimentamos com os dados de treinamento e realizamos o treinamento. Avaliamos então a performance da rede com dados de teste e repetimos todo o processo até alcançar o nível de acurácia desejado em nosso projeto. Quando o modelo final estiver pronto, usamos para realizar as previsões para as quais o modelo foi criado, apresentando a ele novos conjuntos de dados e extraindo as previsões. Perceba que este é um algoritmo de rede neural bem simples, mas que permite compreender como funcionam as redes neurais e mais tarde,  as redes neurais profundas ou Deep Learning.\n",
    "\n",
    "Para finalizar, vamos adicionar um método para retornar a derivada da sigmóide e os métodos *avalia* e *derivada_custo*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def avalia(self, teste):\n",
    "        resultado_teste = [(np.argmax(self.feedForward(x)), y) for (x, y) in teste]\n",
    "        return sum(int(x == y) for (x, y) in resultado_teste)\n",
    "\n",
    "    def derivada_custo(self, output_activations, y):\n",
    "        return (output_activations-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, o código completo da classe RedeNeural:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class RedeNeural:\n",
    "    def __init__(self, tamanhos):\n",
    "        self.ncamadas = len(tamanhos)\n",
    "        self.tamanhos = tamanhos\n",
    "        self.bias  = [np.random.randn(y, 1) for y in tamanhos[1:]]\n",
    "        self.pesos = [np.random.randn(y, x) for x, y in zip(tamanhos[:-1], tamanhos[1:])]\n",
    "    \n",
    "    def sigmoide(self, z):\n",
    "        return 1./(1. + np.exp(-z))\n",
    "    \n",
    "    def sigmoide_prim(self, z):\n",
    "        return self.sigmoide(z)*(1-self.sigmoide(z))\n",
    "    \n",
    "    def feedForward(self, a):\n",
    "        for b, w in zip(self.bias, self.pesos):\n",
    "            a = self.sigmoide(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, treino, epocas, tamanho_lote, eta, teste=None):\n",
    "        #import ipdb; ipdb.set_trace()\n",
    "        treino = list(treino)\n",
    "        n = len(treino)\n",
    "        \n",
    "        if teste:\n",
    "            teste = list(teste)\n",
    "            n_teste = len(teste)\n",
    "        \n",
    "        for j in range(epocas):\n",
    "            random.shuffle(treino)\n",
    "            lotes = [treino[k:k+tamanho_lote] for k in range(0, n, tamanho_lote)]\n",
    "            \n",
    "            for lote in lotes:\n",
    "                self.atualiza_lote(lote, eta)\n",
    "                \n",
    "            if teste:\n",
    "                print(\"Época {} : {}/{}\".format(j, self.avalia(teste), n_teste));\n",
    "            else:\n",
    "                print(\"Época {} finalizada\".format(j))\n",
    "                \n",
    "    def atualiza_lote(self, lote, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.bias]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.pesos]\n",
    "        \n",
    "        for x, y in lote:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        self.pesos = [w-(eta/len(lote))*nw for w, nw in zip(self.pesos, nabla_w)]\n",
    "        self.bias  = [b-(eta/len(lote))*nb for b, nb in zip(self.bias, nabla_b)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.bias]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.pesos]\n",
    "        \n",
    "        # Feedforward\n",
    "        ativacao = x\n",
    "        \n",
    "        # Lista para armazenar todas as ativações, camada por camada\n",
    "        ativacoes = [x]\n",
    "        \n",
    "        # Lista par armazenar todos os vetores z, camada por camada\n",
    "        zs = []\n",
    "        \n",
    "        for b, w in zip(self.bias, self.pesos):\n",
    "            z = np.dot(w, ativacao) + b\n",
    "            zs.append(z)\n",
    "            ativacao = self.sigmoide(z)\n",
    "            ativacoes.append(ativacao)\n",
    "        \n",
    "        # Backward pass\n",
    "        delta = self.derivada_custo(ativacoes[-1], y) * self.sigmoide_prim(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, ativacoes[-2].transpose())\n",
    "        \n",
    "        for l in range(2, self.ncamadas):\n",
    "            z = zs[-l]\n",
    "            sp = self.sigmoide_prim(z)\n",
    "            delta = np.dot(self.pesos[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, ativacoes[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def avalia(self, teste):\n",
    "        resultado_teste = [(np.argmax(self.feedForward(x)), y) for (x, y) in teste]\n",
    "        return sum(int(x == y) for (x, y) in resultado_teste)\n",
    "\n",
    "    def derivada_custo(self, output_activations, y):\n",
    "        return (output_activations-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos carregar os dados de treinamento, que estão no diretório **data**. Vamos também construir algumas funções auxiliares, para prepará-los no formato que nossa rede aceita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset MNIST\n",
    "\n",
    "# Imports\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    f = gzip.open('data/mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = list(zip(validation_inputs, va_d[1]))\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = list(zip(test_inputs, te_d[1]))\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora dividir nosso conjunto de dados em três partes, um conjunto de treinamento, um de validação e um de teste. Vamos olhar o formato do primeiro elemento, para ver se está da mesma forma que necessitamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treino, valida, teste = load_data_wrapper()\n",
    "treino[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quisermos visualizar os dados, podemos utilizar a biblioteca Matplotlib para isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "plt.figure(figsize=(2,2));\n",
    "m = treino[0][0].reshape((28,28))\n",
    "plt.imshow(m);\n",
    "\n",
    "def prev_to_img(ind):\n",
    "    plt.figure(figsize=(2,2));\n",
    "    m = teste[ind][0].reshape((28,28))\n",
    "    plt.imshow(m);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chegou o momento! Vamos construir uma nova rede com as configurações que determinamos no projeto e treiná-la por 100 épocas. Hora de um pouco de paciência..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 0 finalizada\n"
     ]
    }
   ],
   "source": [
    "# Uma tupla com o número de neurônios em cada camada.\n",
    "# O número de camadas é igual ao número de elementos da tupla.\n",
    "rede = RedeNeural((784,28,14,7,10))\n",
    "#SGD(self, treino, epocas, tamanho_lote, eta, teste=None):\n",
    "rede.SGD(treino, 10, 100, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bom! Agora, podemos testar o desempenho usando nosso conjunto de teste. Vamos limitar o número de casas decimais em 2, para ficar mais fácil de observar. Não se preocupe, é apenas para exibição. O Numpy armazena o número completo, portanto, não há perda de precisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  ]\n",
      " [0.16]\n",
      " [0.04]\n",
      " [0.32]\n",
      " [0.01]\n",
      " [0.07]\n",
      " [0.  ]\n",
      " [0.01]\n",
      " [0.28]\n",
      " [0.11]]\n",
      "Previsão: 3\n",
      "Real:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Este comando configura a exibição para duas casas decimais\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "# Índice da amostra a testar\n",
    "ind = 2348\n",
    "# Apresentando uma amostra à rede e capturando a previsão\n",
    "res = rede.feedForward(teste[ind][0])\n",
    "print(res)\n",
    "\n",
    "# Mostrando a amostra original\n",
    "prev_to_img(ind)\n",
    "print(\"Previsão: {}\".format(np.argmax(res)))\n",
    "print(\"Real:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Será que nossa rede está com bom desempenho? Uma das técnicas mais usadas para aferir a qualidade de um classificador é a Matriz de Confusão. Nesta matriz, confrontamos o valor esperado nas linhas com o valor previsto pelo classificador nas colunas. Na diagonal principal, ficam todos os acertos, ou seja, todas as vezes que nosso classificador acertou o dígito. Fora das diagonais, ficam os erros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4551    0   84   14   27   66   88   44   50    8]\n",
      " [   1 5414   90   27   17   17   34   32   39    7]\n",
      " [ 161  104 3772  115  112   20  248  149  252   35]\n",
      " [ 141  127  613 2156  223  681  190  218  608  144]\n",
      " [  40   76  239  130 3210   95  231   86  147  605]\n",
      " [ 208  139  121  264  172 2744  404   69  253  132]\n",
      " [  76   57  177   49   75   95 4348   19   50    5]\n",
      " [  39   91   79  114  150   74   13 4328   63  224]\n",
      " [  73  177  180  646  103  279  117   66 3058  143]\n",
      " [  67   50   69  275  726  102   23  339  112 3225]]\n"
     ]
    }
   ],
   "source": [
    "confMat = np.zeros((10,10), dtype=int)\n",
    "for t in treino:\n",
    "    prev = np.argmax(rede.feedForward(t[0]))\n",
    "    espr = np.argmax(t[1])\n",
    "    confMat[espr][prev] += 1;\n",
    "print(confMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, vamos calcular um indicador chamado acurácia. A acurácia permite um vislumbre rápido do desempenho do classificador, pois mostra a razão entre o número de acertos e o número total de amostras. Mais cuidado! Apenas a acurácia não permite visualizar corretamente o desempenho do classificador! Outras métricas devem ser usadas em conjunto com ela, para termos uma visão mais completa. Veja a figura. O estudo dessas métricas fica como exercício ;-)\n",
    "\n",
    "![](acc_prec.png)\n",
    "\n",
    "Vamos construir uma função que imprime um relatório com a acurácia calculada para cada classe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relatório de métricas:\n",
      "Acurácia média: 0.00\n",
      "Acurácia por Classe:\n",
      "Classe  0: 0.9763\n",
      "Classe  1: 0.9783\n",
      "Classe  2: 0.9430\n",
      "Classe  3: 0.9084\n",
      "Classe  4: 0.9349\n",
      "Classe  5: 0.9362\n",
      "Classe  6: 0.9610\n",
      "Classe  7: 0.9626\n",
      "Classe  8: 0.9328\n",
      "Classe  9: 0.9387\n"
     ]
    }
   ],
   "source": [
    "def relatorio(confMat):\n",
    "    l = confMat.shape[0]\n",
    "    accPorClasse = []\n",
    "    for i in range(l):\n",
    "        tp = confMat[i][i]\n",
    "        fp = np.sum(confMat, axis=1)[i] - tp\n",
    "        fn = np.sum(confMat, axis=0)[i] - tp\n",
    "        tn = np.sum(confMat) - tp - fp - fn\n",
    "        accPorClasse.append((tp+tn)/(tp+tn+fp+fn))\n",
    "    print(f\"Relatório de métricas:\")\n",
    "    accAvg = np.average(accPorClasse)\n",
    "    print(\"Acurácia média: \"+f\"{0:4.2f}\".format(accAvg))\n",
    "    print(f\"Acurácia por Classe:\")\n",
    "    for i, v in enumerate(accPorClasse):\n",
    "        print(\"Classe {0:2d}: \".format(i)+\"{0:4.4f}\".format(v))\n",
    "        \n",
    "relatorio(confMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
