{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais Artificiais\n",
    "\n",
    "## Introdução\n",
    "\n",
    "As redes neurais artificiais foram propostas ainda na década de 50, como uma tentativa de se construir um modelo que reproduzisse, ainda que de forma bastante limitada, o comportamento da melhor máquina pensante já criada: o cérebro.\n",
    "\n",
    "O cérebro é um computador biológico, altamente paralelo, capaz de processar sinais analógicos e resolver problemas não-lineares complexos. Considere, como exemplo, a ecolocalização de um morcego. Usando o seu sonar natural, o morcego consegue várias informações, como a distância até um alvo (por exemplo, um inseto), seu tamanho, sua posição relativa ao morcego. Todos esses cálculos complexos acontecem em frações de segundo, em um cérebro do tamanho de uma ameixa. E com uma precisão de dar inveja aos melhores engenheiros de radar. E ao mesmo tempo, cuidando dos batimentos cardiácos e da respiração, enviando sinais para os músculos, etc...\n",
    "\n",
    "Como o cérebro de um morcego é capaz de faze isso? Um cérebro recém nascido tem uma grande estrutura e capacidade de criar suas próprias regras, o que chamamos normalmente de *experiência*. Em humanos, o período de maior desenvolvimento do cérebro ocorre durante os dois primeiros anos de vida, quando se desenvolvem as ligações físicas entre os neurônios, além do desenvolvimento destes. Entretanto, a capacidade de desenvolvimento do cérebro prossegue até muito além disso. A essa capacidade de adaptação do cérebro, chamamos de *plasticidade*.\n",
    "\n",
    "Uma rede neural artificial constrói um modelo de um neurônio e aplica a este uma *função de aprendizagem*, para que este seja capaz de se adaptar a uma tarefa, aprendendo como realizá-la. Para realizar este fim, as RNA empregam uma rede maciçamente conectada de unidades de processamento (os neurônios artificiais) que realizam computação útil a partir de um processo de aprendizagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Características\n",
    "\n",
    "Dentre as muitas características das redes neurais, podemos destacar:\n",
    "\n",
    "1. Adaptação por experiência: a rede adapta-se e aprende a partir da apresentação sucessiva de exemplos.\n",
    "2. Capacidade de aprendizagem: a rede é capaz de extrair o relacionamento entre as diversas variáveis de entrada da aplicação.\n",
    "3. Habilidade de generalização: através dos exemplos de entrada e do aprendizado decorrido da exposição a estes, a rede é capaz de estimar uma solução para um conjunto de entradas desconhecido.\n",
    "4. Organização de dados: a rede é capaz de se organizar internamente visando agrupamento de padrões e particularidades.\n",
    "5. Tolerância a falhas: uma rede altamente interconectada torna-se tolerante a falhas, por implicar em um alto nível de redundância."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O neurônio artificial\n",
    "\n",
    "Para entender uma rede neural e suas unidades de processamento, os neurônios artificiais, devemos antes recordar como é a estrutura do cérebro. O cérebro é composto por bilhões de células capazes de transmitir sinais entre si. Um neurônio possui um corpo celular, onde está o núcleo da célula, um axônio, que é uma extensão do corpo celular (a cauda) e um conjunto de extensões no próprio corpo celula, os dendritos. Os sinais são transmitidos de uma célula a outra através de sinais eletroquímicos, carregados através de substâncias químicas especiais, os neurotransmissores. Os impulsos são transmitidos dos axônios para os dendritos, ou seja, os dendritos são a interface de entrada e os axônios a interface de saída. Uma conexão entre dois ou mais neurônios constitui uma sinapse. Os sinais que os dendritos recebem são acumulados e quando passam de um certo limiar (50mV), ele é propagado pelo axônio. Cada sinapse é multiplicada por um peso, que pode aumentar ou dimimuir o valor do sinal de entrada (sinal excitatório ou inibidor).\n",
    "\n",
    "![](rede_neural_biologica.png)\n",
    "\n",
    "O modelo do neurônio artificial segue este princípio. Ele representa os dendritos como um conjunto de valores de entrada e o axônio por um valor de saída. Os componentes dde um neurônio artificial são:\n",
    "\n",
    "1. **Sinais de entrada** ${x1, x2, ..., xn}$: Sãos os sinais ou medidas externos que representam valores assumidos pelas variáveis da aplicação;\n",
    "2. **Pesos sinápticos** ${w_1, w_2, ..., w_n}$: São os valores que servirão para ponderar cada uma das neurônio de McCulloch-Pitts (Haykin, 1994) variáveis de entrada da rede, permitindo qualificar a relevância em relação as funcionalidades do neurônio;\n",
    "3. **Combinador Linear** (Junção de Somatórios - $\\Sigma$): Também chamado de Função Soma, sua função é agregar os sinais de entrada produzindo um valor potencial de ativação.\n",
    "4. **Limiar de ativação** $(\\theta)$: É uma variável que identifica o patamar apropriado para o resultado produzido pelo combinador linear. Comumente, usamos o valor 0 para o limiar, assim o neurônio ativa quando o potencial de ativação é positivo.\n",
    "5. **Potencial de ativação** ${u_n}$: É o resultado produzido pelo avaliação do limiar de ativação e o combinador linear. Caso u ≥ θ, então o neurônio produz um potencial excitatório.\n",
    "6. **Função de ativação** $g(.)$ ou $\\phi(.)$: Seu objetivo é limitar a saída do neurônio dentro de um intervalo de valores assumidos pela arquitetura da rede.\n",
    "7. **Sinal de saída** ${y_n}$: Consiste no valor final produzido pelo neurônio em relação a um determinado conjunto de entrada.\n",
    "\n",
    "![](neuronio_artificial.png)\n",
    "\n",
    "Matematicamente, podemos representar o neurônio através do par de equações abaixo:\n",
    "\n",
    "$$ u_n = \\sum^{N}_{n=1}x_n * w_n - \\theta $$\n",
    "\n",
    "$$ y_n = \\phi(u_n + b) $$\n",
    "\n",
    "Onde o termo $b$ representa o viés (bias), que tem a função de aumentar ou diminuir o valor líquido da entrada, dependendo de seu sinal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de ativação\n",
    "\n",
    "Diferentes funções de ativação podem ser usadas em um neurônio artificial.\n",
    "\n",
    "1. **Função limiar**\n",
    "\n",
    "Esta é a função mais simples. A saída do neurônio será 0, se o valor do potencial de ativação é menor que o limiar, ou 1, caso contrário.\n",
    "\n",
    "$$ \\phi(u_n) = \\begin{cases}\n",
    "  1, & \\mbox{se} & u_n \\geq  \\theta \\\\\n",
    "  0, & \\mbox{se} & u_n < \\theta \\end{cases} $$\n",
    "  \n",
    "  ![](limiar.png)\n",
    "  \n",
    "2. **Função linear por partes**\n",
    "\n",
    "A função linear por partes pode ser entendida como uma aproximação linear de uma função não-linear, onde existe uma região de amplificação responde linearmente ao sinal. Fora dessa região, temos uma saturação do sinal. Caso o sinal não sature, temos um *combinador linear*. Caso o fator de amplificação seja infinitesimalmente grande, a função transforma-se na função limiar.\n",
    "\n",
    "$$ \\phi(u_n) = \\begin{cases}\n",
    "  1, & \\mbox{se} & u_n \\geq 0.5 \\\\\n",
    "  u_n, & \\mbox{se} & -0.5 < u_n < 0.5 \\\\\n",
    "  0, & \\mbox{se} & u_n \\leq -0.5 \\end{cases} $$\n",
    "  \n",
    "  ![](linearpartes.png)\n",
    "  \n",
    "3. **Função Sigmóide**\n",
    "\n",
    "A função sigmóide é a mais usada como função de ativação em RNAs. Ela tem a forma de S e apresenta um comportamento balanceado entre regiões lineares e não-lineares, além de ser estritamente crescente.\n",
    "\n",
    "$$\\phi(u_n) = \\frac{1}{1 + \\exp(-\\alpha u_n)}$$\n",
    "\n",
    "Onde $\\alpha$ é um parâmetro que controla a inclinação da função.\n",
    "\n",
    "![](sigmoide.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O Perceptron\n",
    "\n",
    "O Perceptron é a forma mais simples de RNA, criado nos anos 50, que utiliza uma forma também simples de aprendizado. \n",
    "\n",
    "Usa-se um conjunto de exemplos de treinamento que dão a saída desejada para uma unidade, dado um conjunto de entradas (treinamento supervisionado). O objetivo é aprender pesos sinápticos de tal forma que a unidade de saída produza o valor correta pra cada exemplo. O algoritmo faz atualizações iterativamente até chegar aos pesos corretos. Para avaliar a quantidade do erro, calcula-se a diferença entre o valor correto e o valor previsto pela rede, usando o método dos mínimos quadrados. A cada passo da iteração, o valor dos pesos sinápticos é alterado, até que o número de iterações atinja um limite pré-determinado ou o valor do erro global fique menor que um certo limiar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "universe = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelando um perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    SIGMOID = 0\n",
    "    STEP = 1\n",
    "    TANH = 2\n",
    "    pesos = None\n",
    "    func_ativ = 0\n",
    "    theta = 0.\n",
    "    def __init__(self, n_inputs = 1, func_ativ = 0):\n",
    "        self.pesos = np.random.random(n_inputs)\n",
    "        self.func_ativ = func_ativ\n",
    "    def avalia(self, input, derivative=False):\n",
    "        out = np.dot(input,self.pesos).sum()\n",
    "        if self.func_ativ == 0:\n",
    "            out = self.sigmoid(out, derivative)\n",
    "        elif self.func_ativ == 1:\n",
    "            out = self.step(out)\n",
    "        elif self.func_ativ == 2:\n",
    "            out = self.tanh(out)\n",
    "        print(out)\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        return x * (1-x) if derivative else 1/(1 + np.exp(-x))\n",
    "    def step(self, x):\n",
    "        return 0. if x > self.theta else 1\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95749703 0.05299215 0.7608194 ]\n",
      "0.9900457839677486\n",
      "0.0\n",
      "0.9997978425914383\n"
     ]
    }
   ],
   "source": [
    "P = Perceptron(3)\n",
    "\n",
    "print(P.pesos)\n",
    "sample = np.array([2.4, 1.8, 2.9])\n",
    "P.avalia(sample)\n",
    "P.func_ativ = 1\n",
    "P.avalia(sample)\n",
    "P.func_ativ = 2\n",
    "P.avalia(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outras funções de ativação\n",
    "\n",
    "Às vezes, pode ser desejável que a saída da função de ativação seja uma função antissimétrica impar, centrada no limiar. No lugar das funções definidas anteriormente, que dão saídas entre 0 e 1, usaremos funções cuja saída situa-se entre -1 e 1.\n",
    "\n",
    "A função limiar fica definida desta forma:\n",
    "\n",
    "$$ \n",
    "\\phi(u_n) = \n",
    "  \\begin{cases}\n",
    "    1, & \\mbox{se} & u_n \\geq \\theta \\\\\n",
    "    0, & \\mbox{se} & u_n = \\theta \\\\\n",
    "   -1, & \\mbox{se} & u_n \\leq \\theta\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "![](limiarI.png)\n",
    "   \n",
    "Da mesa forma, a função ímpar análoga à função sigmóide é a função **tangente hiperbólica (tanh)**, definida como:\n",
    "\n",
    "$$\n",
    "\\phi(u_n) = \\tanh(u_n)\n",
    "$$\n",
    "![](tanh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O Perceptron e operações lógicas\n",
    "\n",
    "O Perceptron pode ser pensado como uma maneira de pesar evidências para a tomada de uma decisão. Porém, ele também pode ser usado para resolver funções lógicas (AND, OR, NOT, etc.). Considere o seguinte perceptron com duas variáveis de entrada binárias, $x_1$ e $x_2$, com pesos sinápticos iguais a -2 e bias 3:\n",
    "\n",
    "![](perceptronAND.png)\n",
    "\n",
    "Agora, vamos construir uma tabela com as saídas possíveis para cada entrada:\n",
    "\n",
    "| $x_1$ \t| $x_2$ \t| u_n                      \t| y \t|\n",
    "|-------\t|-------\t|--------------------------\t|---\t|\n",
    "| 0     \t| 0     \t| 0*(-2) + 0*(-2) + 3 = 3  \t| 1 \t|\n",
    "| 0     \t| 1     \t| 0*(-2) + 1*(-2) + 3 = 1  \t| 1 \t|\n",
    "| 1     \t| 0     \t| 1*(-2) + 0*(-2) + 3 = 1  \t| 1 \t|\n",
    "| 1     \t| 1     \t| 1*(-2) + 1*(-2) + 3 = -1 \t| 0 \t|\n",
    "\n",
    "Observe que a tabela representa a tabela verdade da operação lógica NAND! As portas NAND são cruciais para a computação, porque elas podem ser usadas para construir qualquer operação lógica ([confira](https://en.wikipedia.org/wiki/Sheffer_stroke)). Desta forma, podemos usar redes neurais para implementar qualquer operação lógica também.\n",
    "\n",
    "Uma rede de Perceptrons pode ser usada para simular um circuito contendo muitos portões NAND. E como os portões NAND são universais para a computação, segue-se que os Perceptrons também são universais para a computação. Considerando que o Perceptron é o modelo mais simples de rede neural, imagine o que pode ser feito com modelos bem mais avançados! Acertou se você pensou em Inteligência Artificial.\n",
    "\n",
    "A universalidade computacional dos Perceptrons é simultaneamente reconfortante e decepcionante. É reconfortante porque nos diz que redes de Perceptrons podem ser tão poderosas como qualquer outro dispositivo de computação. Mas também é decepcionante, porque parece que os Perceptrons são meramente um novo tipo de portão NAND. Isso não é uma grande noticia!\n",
    "\n",
    "No entanto, a situação é melhor do que esta visão sugere. Acontece que podemos conceber algoritmos de aprendizado que podem ajustar automaticamente os pesos e os vieses de uma rede de neurônios artificiais. Este ajuste ocorre em resposta a estímulos externos, sem intervenção direta de um programador. Esses algoritmos de aprendizagem nos permitem usar neurônios artificiais de uma maneira que é radicalmente diferente dos portões lógicos convencionais. Em vez de colocar explicitamente um circuito de NAND e outros portões, nossas redes neurais podem simplesmente aprender a resolver problemas, às vezes problemas em que seriam extremamente difíceis de projetar diretamente usando um circuito convencional de lógica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operações Lógicas e Regiões Linearmente Separáveis\n",
    "\n",
    "Conforme mencionado acima, um Perceptron calcula a soma ponderada dos valores de entrada. Por simplicidade, suponhamos que existem dois valores de entrada, $x$ e $y$ para um certo Perceptron $P$. Vamos definir os pesos de $x$ e $y$, como sendo $A$ e $B$, respectivamente. A soma ponderada pode ser representada como: $Ax + By$.\n",
    "\n",
    "Uma vez que o Perceptron produz um valor não-zero somente quando a soma ponderada excede um certo limite $C$, pode-se escrever a saída deste Perceptron da seguinte maneira:\n",
    "\n",
    "$$\n",
    "P = \n",
    "\\begin{cases}\n",
    "  1, & se & Ax + By > C\\\\\n",
    "  0, & se & Ax + By <= C\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Considerando que $Ax + By > C$ e $Ax + By < C$ são as duas regiões no plano $xy$ separadas pela linha $Ax + By + C = 0$, e se considerarmos ainda a entrada $(x, y)$ como um ponto em um plano, então o Perceptron realmente nos diz qual região no plano a que esse ponto pertence. Tais regiões, uma vez que são separadas por uma única linha, são chamadas de regiões linearmente separáveis.\n",
    "\n",
    "Um único Perceptron consegue resolver somente funções linearmente separáveis. Em funções não linearmente separáveis, o Perceptron não consegue gerar um hiperplano, esta linha nos gráficos abaixo, para separar os dados. A questão é que no mundo real raramente os dados são linearmente separáveis, fazendo com o que o Perceptron não seja muito útil para atividades práticas (mas sendo ideal para iniciar o estudo em redes neurais artificiais). E como separamos os dados não linearmente separáveis? Através de redes com arquiteturas mais complexas, que farão a combinação de diversas funções.\n",
    "\n",
    "![](linsep_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquiteturas de redes neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, como na prática, vamos lidar com problemas linearmente separáveis, o uso de uma rede de neurônios é crucial. Suponha, então, uma rede de neurônios conectados como abaixo:\n",
    "\n",
    "![](rede.png)\n",
    "\n",
    "A camada mais à esquerda nesta rede é chamada de camada de entrada e os neurônios dentro da camada são chamados de neurônios de entrada. A camada mais à direita ou a saída contém os neurônios de saída ou, como neste caso, um único neurônio de saída. A camada do meio é chamada de camada oculta, já que os neurônios nessa camada não são entradas ou saídas.\n",
    "\n",
    "A rede acima tem apenas uma única camada oculta, mas algumas redes possuem múltiplas camadas ocultas. Por exemplo, a seguinte rede de quatro camadas tem duas camadas ocultas:\n",
    "\n",
    "![](rede2.png)\n",
    "\n",
    "Tais redes de camadas múltiplas são chamados de Perceptrons Multicamadas ou MLPs (Multilayer Perceptrons), ou seja, uma rede neural formada por Perceptrons (embora na verdade seja uma rede de neurônios sigmóides, como veremos mais adiante).\n",
    "\n",
    "O design das camadas de entrada e saída em uma rede geralmente é direto. Por exemplo, suponha que estamos tentando determinar se uma imagem manuscrita representa um “9” ou não. Uma maneira natural de projetar a rede é codificar as intensidades dos pixels da imagem nos neurônios de entrada. Se a imagem for uma imagem em escala de cinza 64 x 64, teríamos 64 × 64 = 4.096  neurônios de entrada, com as intensidades dimensionadas adequadamente entre 0 e 1. A camada de saída conterá apenas um único neurônio com valores inferiores a 0,5 indicando que “a imagem de entrada não é um 9” e valores maiores que 0,5 indicando que “a imagem de entrada é um 9”.\n",
    "\n",
    "Embora o design das camadas de entrada e saída de uma rede neural seja frequentemente direto, pode haver bastante variação para o design das camadas ocultas. Em particular, não é possível resumir o processo de design das camadas ocultas com poucas regras simples. Em vez disso, pesquisadores de redes neurais desenvolveram muitas heurísticas de design para as camadas ocultas, que ajudam as pessoas a obter o comportamento que querem de suas redes. \n",
    "\n",
    "Para se ter uma ideia de como o campo evolui, observe a imagem abaixo, que compila diversas propostas de arquiteturas para redes neurais.\n",
    "\n",
    "![](rnazoo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorias de redes neurais\n",
    "\n",
    "Geralmente, classificamos as arquiteturas de redes neurais em 3 categorias.\n",
    "\n",
    "#### Redes alimentadas à frente (feed-forward)\n",
    "\n",
    "Estes são o tipo mais comum de rede neural em aplicações práticas. A primeira camada é a entrada e a última camada é a saída. Se houver mais de uma camada oculta, nós as chamamos de redes neurais “profundas” (ou Deep Learning). Esses tipos de redes neurais calculam uma série de transformações que alteram as semelhanças entre os casos. As atividades dos neurônios em cada camada são uma função não-linear das atividades na camada anterior.\n",
    "\n",
    "![](ffrna.ppm)\n",
    "\n",
    "#### Redes recorrentes\n",
    "\n",
    "Estes tipos de redes neurais têm ciclos direcionados em seu grafo de conexão. Isso significa que às vezes você pode voltar para onde você começou seguindo as setas. Eles podem ter uma dinâmica complicada e isso pode torná-los muito difíceis de treinar. Entretanto, estes tipos são mais biologicamente realistas.\n",
    "\n",
    "Atualmente, há muito interesse em encontrar formas eficientes de treinamento de redes recorrentes. As redes neurais recorrentes são uma maneira muito natural de modelar dados sequenciais. Eles são equivalentes a redes muito profundas com uma camada oculta por fatia de tempo; exceto que eles usam os mesmos pesos em cada fatia de tempo e recebem entrada em cada fatia. Eles têm a capacidade de lembrar informações em seu estado oculto por um longo período de tempo, mas é muito difícil treiná-las para usar esse potencial.\n",
    "\n",
    "![](recrna.png)\n",
    "\n",
    "#### Redes conectadas simetricamente\n",
    "\n",
    "Estas são como redes recorrentes, mas as conexões entre as unidades são simétricas (elas têm o mesmo peso em ambas as direções). As redes simétricas são muito mais fáceis de analisar do que as redes recorrentes. Elas também são mais restritas no que podem fazer porque obedecem a uma função de energia. \n",
    "\n",
    "![](simrna.png)\n",
    "\n",
    "Dentre estas 3 categorias, podemos listar 10 arquiteturas principais de redes neurais:\n",
    "\n",
    "- Redes Multilayer Perceptron\n",
    "- Redes Neurais Convolucionais\n",
    "- Redes Neurais Recorrentes\n",
    "- Long Short-Term Memory (LSTM)\n",
    "- Redes de Hopfield\n",
    "- Máquinas de Boltzmann\n",
    "- Deep Belief Network\n",
    "- Deep Auto-Encoders\n",
    "- Generative Adversarial Network\n",
    "- Deep Neural Network Capsules (este é um tipo completamente novo de rede neural, lançado no final de 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
